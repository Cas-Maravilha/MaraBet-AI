"""
Sistema de Bootstrap para Confian√ßa - MaraBet AI
Implementa√ß√£o de bootstrap para c√°lculo de intervalos de confian√ßa
"""

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass
import scipy.stats as stats
from sklearn.utils import resample
import warnings

logger = logging.getLogger(__name__)

@dataclass
class BootstrapResult:
    """Resultado do bootstrap"""
    original_statistic: float
    bootstrap_statistics: np.ndarray
    confidence_interval: Tuple[float, float]
    confidence_level: float
    bias: float
    standard_error: float
    bootstrap_samples: int
    method: str

@dataclass
class BootstrapMetrics:
    """M√©tricas do bootstrap"""
    mean_bias: float
    bias_std: float
    coverage_rate: float
    interval_width: float
    efficiency: float
    stability: float

class BootstrapConfidence:
    """
    Sistema de bootstrap para c√°lculo de intervalos de confian√ßa
    Implementa m√∫ltiplas variantes do bootstrap
    """
    
    def __init__(self, 
                 n_bootstrap: int = 1000,
                 random_state: int = 42,
                 confidence_level: float = 0.95):
        """
        Inicializa o sistema de bootstrap
        
        Args:
            n_bootstrap: N√∫mero de amostras bootstrap
            random_state: Seed para reprodutibilidade
            confidence_level: N√≠vel de confian√ßa padr√£o
        """
        self.n_bootstrap = n_bootstrap
        self.random_state = random_state
        self.confidence_level = confidence_level
        np.random.seed(random_state)
        
        logger.info(f"BootstrapConfidence inicializado - {n_bootstrap} amostras, {confidence_level*100:.0f}% confian√ßa")
    
    def bootstrap_confidence_interval(self,
                                    data: Union[List[float], np.ndarray],
                                    statistic_func: callable = np.mean,
                                    method: str = "percentile",
                                    confidence_level: Optional[float] = None) -> BootstrapResult:
        """
        Calcula intervalo de confian√ßa usando bootstrap
        
        Args:
            data: Dados para bootstrap
            statistic_func: Fun√ß√£o estat√≠stica a ser calculada
            method: M√©todo de bootstrap ("percentile", "bias_corrected", "studentized")
            confidence_level: N√≠vel de confian√ßa
            
        Returns:
            Resultado do bootstrap
        """
        try:
            if confidence_level is None:
                confidence_level = self.confidence_level
            
            data = np.array(data)
            original_statistic = statistic_func(data)
            
            # Gerar amostras bootstrap
            bootstrap_statistics = self._generate_bootstrap_samples(
                data, statistic_func, self.n_bootstrap
            )
            
            # Calcular intervalo de confian√ßa baseado no m√©todo
            if method == "percentile":
                ci_lower, ci_upper = self._percentile_confidence_interval(
                    bootstrap_statistics, confidence_level
                )
            elif method == "bias_corrected":
                ci_lower, ci_upper = self._bias_corrected_confidence_interval(
                    bootstrap_statistics, original_statistic, confidence_level
                )
            elif method == "studentized":
                ci_lower, ci_upper = self._studentized_confidence_interval(
                    data, statistic_func, bootstrap_statistics, confidence_level
                )
            else:
                raise ValueError(f"M√©todo n√£o suportado: {method}")
            
            # Calcular m√©tricas
            bias = np.mean(bootstrap_statistics) - original_statistic
            standard_error = np.std(bootstrap_statistics)
            
            return BootstrapResult(
                original_statistic=original_statistic,
                bootstrap_statistics=bootstrap_statistics,
                confidence_interval=(ci_lower, ci_upper),
                confidence_level=confidence_level,
                bias=bias,
                standard_error=standard_error,
                bootstrap_samples=self.n_bootstrap,
                method=method
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erro no bootstrap: {e}")
            return self._empty_bootstrap_result()
    
    def _generate_bootstrap_samples(self,
                                  data: np.ndarray,
                                  statistic_func: callable,
                                  n_bootstrap: int) -> np.ndarray:
        """Gera amostras bootstrap"""
        try:
            bootstrap_statistics = []
            
            for _ in range(n_bootstrap):
                # Amostragem com reposi√ß√£o
                bootstrap_sample = resample(data, random_state=self.random_state)
                bootstrap_statistic = statistic_func(bootstrap_sample)
                bootstrap_statistics.append(bootstrap_statistic)
            
            return np.array(bootstrap_statistics)
            
        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o de amostras bootstrap: {e}")
            return np.array([])
    
    def _percentile_confidence_interval(self,
                                      bootstrap_statistics: np.ndarray,
                                      confidence_level: float) -> Tuple[float, float]:
        """Calcula intervalo de confian√ßa usando m√©todo percentil"""
        try:
            alpha = 1 - confidence_level
            lower_percentile = (alpha / 2) * 100
            upper_percentile = (1 - alpha / 2) * 100
            
            ci_lower = np.percentile(bootstrap_statistics, lower_percentile)
            ci_upper = np.percentile(bootstrap_statistics, upper_percentile)
            
            return ci_lower, ci_upper
            
        except Exception as e:
            logger.error(f"‚ùå Erro no m√©todo percentil: {e}")
            return 0.0, 0.0
    
    def _bias_corrected_confidence_interval(self,
                                          bootstrap_statistics: np.ndarray,
                                          original_statistic: float,
                                          confidence_level: float) -> Tuple[float, float]:
        """Calcula intervalo de confian√ßa usando m√©todo bias-corrected"""
        try:
            # Calcular vi√©s
            bias = np.mean(bootstrap_statistics) - original_statistic
            
            # Ajustar estat√≠sticas bootstrap
            bias_corrected_statistics = bootstrap_statistics - bias
            
            # Calcular percentis
            alpha = 1 - confidence_level
            lower_percentile = (alpha / 2) * 100
            upper_percentile = (1 - alpha / 2) * 100
            
            ci_lower = np.percentile(bias_corrected_statistics, lower_percentile)
            ci_upper = np.percentile(bias_corrected_statistics, upper_percentile)
            
            return ci_lower, ci_upper
            
        except Exception as e:
            logger.error(f"‚ùå Erro no m√©todo bias-corrected: {e}")
            return 0.0, 0.0
    
    def _studentized_confidence_interval(self,
                                       data: np.ndarray,
                                       statistic_func: callable,
                                       bootstrap_statistics: np.ndarray,
                                       confidence_level: float) -> Tuple[float, float]:
        """Calcula intervalo de confian√ßa usando m√©todo studentized"""
        try:
            original_statistic = statistic_func(data)
            
            # Calcular erro padr√£o bootstrap
            bootstrap_se = np.std(bootstrap_statistics)
            
            # Calcular estat√≠sticas t bootstrap
            t_statistics = (bootstrap_statistics - original_statistic) / bootstrap_se
            
            # Calcular percentis das estat√≠sticas t
            alpha = 1 - confidence_level
            lower_percentile = (alpha / 2) * 100
            upper_percentile = (1 - alpha / 2) * 100
            
            t_lower = np.percentile(t_statistics, lower_percentile)
            t_upper = np.percentile(t_statistics, upper_percentile)
            
            # Calcular intervalo de confian√ßa
            ci_lower = original_statistic - t_upper * bootstrap_se
            ci_upper = original_statistic - t_lower * bootstrap_se
            
            return ci_lower, ci_upper
            
        except Exception as e:
            logger.error(f"‚ùå Erro no m√©todo studentized: {e}")
            return 0.0, 0.0
    
    def bootstrap_prediction_interval(self,
                                    predictions: List[float],
                                    actual_values: List[float],
                                    confidence_level: Optional[float] = None) -> BootstrapResult:
        """
        Calcula intervalo de predi√ß√£o usando bootstrap
        
        Args:
            predictions: Lista de previs√µes
            actual_values: Lista de valores reais
            confidence_level: N√≠vel de confian√ßa
            
        Returns:
            Resultado do bootstrap para predi√ß√£o
        """
        try:
            if confidence_level is None:
                confidence_level = self.confidence_level
            
            predictions = np.array(predictions)
            actual_values = np.array(actual_values)
            
            # Calcular erros de predi√ß√£o
            errors = actual_values - predictions
            
            # Bootstrap dos erros
            bootstrap_errors = self._generate_bootstrap_samples(
                errors, np.mean, self.n_bootstrap
            )
            
            # Calcular previs√£o m√©dia
            mean_prediction = np.mean(predictions)
            
            # Calcular intervalo de confian√ßa dos erros
            alpha = 1 - confidence_level
            lower_percentile = (alpha / 2) * 100
            upper_percentile = (1 - alpha / 2) * 100
            
            error_lower = np.percentile(bootstrap_errors, lower_percentile)
            error_upper = np.percentile(bootstrap_errors, upper_percentile)
            
            # Calcular intervalo de predi√ß√£o
            pred_lower = mean_prediction + error_lower
            pred_upper = mean_prediction + error_upper
            
            return BootstrapResult(
                original_statistic=mean_prediction,
                bootstrap_statistics=bootstrap_errors,
                confidence_interval=(pred_lower, pred_upper),
                confidence_level=confidence_level,
                bias=np.mean(bootstrap_errors),
                standard_error=np.std(bootstrap_errors),
                bootstrap_samples=self.n_bootstrap,
                method="prediction"
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erro no bootstrap de predi√ß√£o: {e}")
            return self._empty_bootstrap_result()
    
    def compare_bootstrap_methods(self,
                                data: Union[List[float], np.ndarray],
                                statistic_func: callable = np.mean,
                                confidence_levels: List[float] = [0.68, 0.80, 0.90, 0.95, 0.99]) -> Dict[str, Dict[float, BootstrapResult]]:
        """
        Compara diferentes m√©todos de bootstrap
        
        Args:
            data: Dados para an√°lise
            statistic_func: Fun√ß√£o estat√≠stica
            confidence_levels: N√≠veis de confian√ßa para comparar
            
        Returns:
            Dicion√°rio com resultados por m√©todo e n√≠vel
        """
        try:
            methods = ["percentile", "bias_corrected", "studentized"]
            results = {}
            
            for method in methods:
                method_results = {}
                
                for level in confidence_levels:
                    result = self.bootstrap_confidence_interval(
                        data, statistic_func, method, level
                    )
                    method_results[level] = result
                
                results[method] = method_results
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Erro na compara√ß√£o de m√©todos: {e}")
            return {}
    
    def evaluate_bootstrap_quality(self,
                                 bootstrap_results: List[BootstrapResult],
                                 true_values: Optional[List[float]] = None) -> BootstrapMetrics:
        """
        Avalia qualidade dos resultados bootstrap
        
        Args:
            bootstrap_results: Lista de resultados bootstrap
            true_values: Valores verdadeiros (opcional)
            
        Returns:
            M√©tricas de qualidade do bootstrap
        """
        try:
            if not bootstrap_results:
                return self._empty_bootstrap_metrics()
            
            # Calcular m√©tricas de vi√©s
            biases = [result.bias for result in bootstrap_results]
            mean_bias = np.mean(np.abs(biases))
            bias_std = np.std(biases)
            
            # Calcular taxa de cobertura (se valores verdadeiros dispon√≠veis)
            if true_values and len(true_values) == len(bootstrap_results):
                covered = 0
                for result, true_val in zip(bootstrap_results, true_values):
                    ci_lower, ci_upper = result.confidence_interval
                    if ci_lower <= true_val <= ci_upper:
                        covered += 1
                coverage_rate = covered / len(bootstrap_results)
            else:
                coverage_rate = 0.0
            
            # Calcular largura m√©dia dos intervalos
            interval_widths = []
            for result in bootstrap_results:
                ci_lower, ci_upper = result.confidence_interval
                width = ci_upper - ci_lower
                interval_widths.append(width)
            
            average_width = np.mean(interval_widths)
            
            # Calcular efici√™ncia (inversamente proporcional √† largura)
            efficiency = 100 / (1 + average_width) if average_width > 0 else 0
            
            # Calcular estabilidade (consist√™ncia dos resultados)
            standard_errors = [result.standard_error for result in bootstrap_results]
            stability = 100 - (np.std(standard_errors) / np.mean(standard_errors) * 100) if np.mean(standard_errors) > 0 else 0
            
            return BootstrapMetrics(
                mean_bias=mean_bias,
                bias_std=bias_std,
                coverage_rate=coverage_rate,
                interval_width=average_width,
                efficiency=efficiency,
                stability=stability
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erro na avalia√ß√£o de qualidade: {e}")
            return self._empty_bootstrap_metrics()
    
    def bootstrap_uncertainty_analysis(self,
                                     predictions: List[float],
                                     actual_values: List[float],
                                     confidence_levels: List[float] = [0.68, 0.80, 0.90, 0.95, 0.99]) -> Dict[str, Any]:
        """
        An√°lise completa de incerteza usando bootstrap
        
        Args:
            predictions: Lista de previs√µes
            actual_values: Lista de valores reais
            confidence_levels: N√≠veis de confian√ßa
            
        Returns:
            An√°lise completa de incerteza
        """
        try:
            predictions = np.array(predictions)
            actual_values = np.array(actual_values)
            
            # Calcular erros
            errors = actual_values - predictions
            
            # Bootstrap para diferentes n√≠veis de confian√ßa
            bootstrap_results = {}
            for level in confidence_levels:
                result = self.bootstrap_confidence_interval(
                    errors, np.mean, "percentile", level
                )
                bootstrap_results[level] = result
            
            # Avaliar qualidade
            quality_metrics = self.evaluate_bootstrap_quality(
                list(bootstrap_results.values()), actual_values
            )
            
            # Calcular m√©tricas de incerteza
            uncertainty_metrics = {
                'mean_uncertainty': np.mean([abs(e) for e in errors]),
                'uncertainty_std': np.std([abs(e) for e in errors]),
                'prediction_accuracy': 100 - (np.mean([abs(e) for e in errors]) / np.mean(actual_values) * 100),
                'coverage_rates': {
                    level: self._calculate_coverage_rate(result, actual_values)
                    for level, result in bootstrap_results.items()
                }
            }
            
            return {
                'bootstrap_results': bootstrap_results,
                'quality_metrics': quality_metrics,
                'uncertainty_metrics': uncertainty_metrics,
                'recommendations': self._generate_bootstrap_recommendations(quality_metrics, uncertainty_metrics)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de incerteza bootstrap: {e}")
            return {}
    
    def _calculate_coverage_rate(self, 
                               result: BootstrapResult,
                               actual_values: List[float]) -> float:
        """Calcula taxa de cobertura para um resultado bootstrap"""
        try:
            ci_lower, ci_upper = result.confidence_interval
            covered = sum(1 for val in actual_values if ci_lower <= val <= ci_upper)
            return covered / len(actual_values) * 100
        except Exception as e:
            logger.error(f"‚ùå Erro no c√°lculo de cobertura: {e}")
            return 0.0
    
    def _generate_bootstrap_recommendations(self,
                                          quality_metrics: BootstrapMetrics,
                                          uncertainty_metrics: Dict[str, Any]) -> List[str]:
        """Gera recomenda√ß√µes baseadas na an√°lise bootstrap"""
        try:
            recommendations = []
            
            # Recomenda√ß√µes baseadas no vi√©s
            if quality_metrics.mean_bias > 0.1:
                recommendations.append("‚ö†Ô∏è Alto vi√©s detectado no bootstrap. Considere aumentar o n√∫mero de amostras ou verificar a qualidade dos dados.")
            elif quality_metrics.mean_bias < 0.01:
                recommendations.append("‚úÖ Baixo vi√©s - bootstrap bem calibrado.")
            else:
                recommendations.append("üìä Vi√©s moderado - monitore a qualidade do bootstrap.")
            
            # Recomenda√ß√µes baseadas na cobertura
            if quality_metrics.coverage_rate > 0:
                if quality_metrics.coverage_rate > 95:
                    recommendations.append("üéØ Excelente cobertura - intervalos de confian√ßa precisos.")
                elif quality_metrics.coverage_rate > 90:
                    recommendations.append("‚úÖ Boa cobertura - intervalos de confian√ßa adequados.")
                else:
                    recommendations.append("‚ö†Ô∏è Baixa cobertura - intervalos podem estar subestimados.")
            
            # Recomenda√ß√µes baseadas na efici√™ncia
            if quality_metrics.efficiency > 80:
                recommendations.append("‚ö° Alta efici√™ncia - intervalos precisos e informativos.")
            elif quality_metrics.efficiency > 60:
                recommendations.append("üìä Efici√™ncia moderada - intervalos adequados.")
            else:
                recommendations.append("üîç Baixa efici√™ncia - considere otimizar o m√©todo de bootstrap.")
            
            # Recomenda√ß√µes baseadas na estabilidade
            if quality_metrics.stability > 90:
                recommendations.append("üõ°Ô∏è Alta estabilidade - resultados consistentes.")
            elif quality_metrics.stability > 70:
                recommendations.append("üìà Estabilidade moderada - resultados razoavelmente consistentes.")
            else:
                recommendations.append("‚ö†Ô∏è Baixa estabilidade - considere aumentar o n√∫mero de amostras bootstrap.")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar recomenda√ß√µes: {e}")
            return []
    
    def _empty_bootstrap_result(self) -> BootstrapResult:
        """Retorna resultado bootstrap vazio"""
        return BootstrapResult(
            original_statistic=0.0,
            bootstrap_statistics=np.array([]),
            confidence_interval=(0.0, 0.0),
            confidence_level=0.95,
            bias=0.0,
            standard_error=0.0,
            bootstrap_samples=0,
            method="none"
        )
    
    def _empty_bootstrap_metrics(self) -> BootstrapMetrics:
        """Retorna m√©tricas bootstrap vazias"""
        return BootstrapMetrics(
            mean_bias=0.0,
            bias_std=0.0,
            coverage_rate=0.0,
            interval_width=0.0,
            efficiency=0.0,
            stability=0.0
        )
